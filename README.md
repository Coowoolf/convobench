# PaperHunt

<p align="center">
  <img src="public/logo.jpg" alt="PaperHunt Logo" width="200" />
</p>

**Conversational AI Insight Discovery Tool** â€” 60+ curated research papers spanning 75 years, from Turing Test (1950) to Full-Duplex Real-time Interaction (2026).

ğŸ”— **[paperhunt.org](https://paperhunt.org)**

---

## ğŸ¯ Core Metrics

| Metric | Value | Description |
|--------|-------|-------------|
| **Papers** | 60+ | Curated research collection |
| **Human Turn Gap** | ~200ms | Natural conversation pause |
| **GPT-4o Latency** | ~320ms | Approaching human-level |
| **Target** | Full-Duplex | Listening while Speaking |

---

## ğŸ—ºï¸ Interactive Pipeline Map

Visual tier-based taxonomy for Voice Agent architecture:

| Tier | Category | Description |
|------|----------|-------------|
| **0** | Hall of Fame | Foundation classics (Turing, ELIZA, Transformer) |
| **2** | Ear (ASR) | Audio input processing |
| **3** | Brain (LLM) | Core intelligence layer |
| **4** | Voice (TTS) | Audio output synthesis |
| **5** | Support | Benchmarks & Tools (VoiceAgentEval) |
| **6** | Global | System-level design |

---

## ğŸ”¥ The Experience Gaps

Why typical voice bots feel robotic:

1. **Turn-Taking Gap** â€” 1-3s response delay breaks flow
2. **Barge-In** â€” Can't hear interruptions while speaking  
3. **Prosody & Affect** â€” Text conversion loses emotion
4. **Context Loss** â€” Single models can't handle orchestration

**Solution:** Omni-Duplex Architecture with streaming audio tokens

---

## âœ¨ Features

- ğŸ—ºï¸ **Interactive Pipeline Map** â€” Visual tier-based paper taxonomy
- ğŸ“Š **Core Metrics Dashboard** â€” WER, latency, research trends
- ğŸ¯ **Experience Gaps Analysis** â€” Why bots feel robotic
- ğŸ“… **Historical Timeline** â€” 1950-2026 milestones
- ğŸ” **Smart Search** â€” Filter by era, tier, tags

---

Built with â¤ï¸ for the Voice Agent community.

Â© 2026 PaperHunt
